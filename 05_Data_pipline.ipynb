{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f8b742",
   "metadata": {},
   "source": [
    "هو عبارة عن تجميع كل الخطوات اللي سويناها (EDA → Cleaning → Splitting → Balancing→ Preprocessing) في تسلسل واضح ومرتب"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad008c98",
   "metadata": {},
   "source": [
    "Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be98eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random, json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6050051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# مسار الداتا بعد التقسيم\n",
    "SPLIT_DIR = Path(\"./ArASL_Split\")\n",
    "\n",
    "# نستخدم قناة وحدة (رمادي). خليها True لو بتستخدمين موديلات pretrained (3 قنوات)\n",
    "USE_RGB_3CH = False\n",
    "\n",
    "# مقاس الإدخال (متوافق مع التنظيف السابق)\n",
    "IMG_SIZE   = (64, 64)\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True and torch.cuda.is_available()\n",
    "\n",
    "# ثبّت العشوائية\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909cdf2",
   "metadata": {},
   "source": [
    "Classes mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebafab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classes (32): ['ain', 'al', 'aleff', 'bb', 'dal', 'dha', 'dhad', 'fa', 'gaaf', 'ghain', 'ha', 'haa', 'jeem', 'kaaf', 'khaa', 'la', 'laam', 'meem', 'nun', 'ra', 'saad', 'seen', 'sheen', 'ta', 'taa', 'thaa', 'thal', 'toot', 'waw', 'ya', 'yaa', 'zay']\n"
     ]
    }
   ],
   "source": [
    "#  نقرأ أسماء الكلاسات من train ونثبّت الترتيب -> label ثابت\n",
    "#الهدف: نحصل على قائمة الكلاسات وتعريفها بأرقام ثابتة\n",
    "\n",
    "train_root = SPLIT_DIR / \"train\"\n",
    "assert train_root.exists(), f\"Train folder not found: {train_root}\"\n",
    "\n",
    "classes = sorted([d.name for d in train_root.iterdir() if d.is_dir()])\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
    "print(f\"[INFO] classes ({len(classes)}):\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7f38f",
   "metadata": {},
   "source": [
    "Basic transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d04b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# هنا نخلي التحويلات بسيطة ونفسها للـ train/val/test \n",
    "def _to_tensor_gray_or_rgb():\n",
    "    ops = [transforms.ToTensor()]  # يحوّل إلى Tensor بنطاق [0..1] وشكل CxHxW\n",
    "    if USE_RGB_3CH:\n",
    "        # لو الصورة 1قناة، نوسّعها إلى 3 قنوات (لموديلات 3ch)\n",
    "        ops.append(transforms.Lambda(lambda t: t.expand(3, *t.shape[1:])))\n",
    "    return transforms.Compose(ops)\n",
    "\n",
    "basic_tf = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),     # تأكيد رمادي\n",
    "    transforms.Resize(IMG_SIZE, antialias=True),     # تأكيد المقاس\n",
    "    _to_tensor_gray_or_rgb(),\n",
    "    # Normalize بسيط حول 0.5. تقدرين لاحقًا تحسبين mean/std من train لو حبيتي.\n",
    "    transforms.Normalize(mean=[0.5] * (3 if USE_RGB_3CH else 1),\n",
    "                         std =[0.5] * (3 if USE_RGB_3CH else 1)),\n",
    "])\n",
    "\n",
    "train_tf = basic_tf\n",
    "val_tf   = basic_tf\n",
    "test_tf  = basic_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cabba8",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff084ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "      يقرأ الصور من مجلد split (train/val/test) بشكل:\n",
    "      root/class_name/*.png → (image_tensor, label_int)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: Path, class_to_idx: dict, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "        for cls_name, idx in self.class_to_idx.items():\n",
    "            cls_dir = self.root_dir / cls_name\n",
    "            if not cls_dir.exists():\n",
    "                continue\n",
    "            for p in cls_dir.iterdir():\n",
    "                if p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".bmp\"}:\n",
    "                    self.samples.append((p, idx))\n",
    "\n",
    "        assert len(self.samples) > 0, f\"No images under {self.root_dir}\"\n",
    "        # ترتيب ثابت؛ الشفل يصير في DataLoader\n",
    "        self.samples.sort(key=lambda x: (x[1], x[0].name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.samples[i]\n",
    "        # NOTE (Arabic): نقرأ بـ PIL (أفضل لـ torchvision.transforms)\n",
    "        img = Image.open(path)\n",
    "        # لو الملف أساسًا رمادي/ألوان، Grayscale بالـ transform سيضمن قناة واحدة\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f12b5",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fb89a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train/val/test sizes = 48038 / 4842 / 4842\n"
     ]
    }
   ],
   "source": [
    "train_ds = ArASLDataset(SPLIT_DIR/\"train\", class_to_idx, transform=train_tf)\n",
    "val_ds   = ArASLDataset(SPLIT_DIR/\"val\",   class_to_idx, transform=val_tf)\n",
    "test_ds  = ArASLDataset(SPLIT_DIR/\"test\",  class_to_idx, transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"[INFO] train/val/test sizes = {len(train_ds)} / {len(val_ds)} / {len(test_ds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
